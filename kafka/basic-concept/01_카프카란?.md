![Apache kafka 개요](./images/6a46514d46e0edd07ab4e80c828d60a8.png)



# apache Kafka 



## 카프카란?

### 탄생 배경

미국의 대표적 비지니스 소셜 네트워크 서비스인 링크드인에서 처음 출발한 기술이다. 링크드인 사이트가 급속도로 성장하면서 발생하는 내부 이슈들을 해결하기 위해 탄생했다.

> ✏️ 카프카의 어원
>
> 카프카의 창시자인 제이 크렙스는 대학 시절 문학 수업을 들으며 소설가 프란츠 카프카에 심취했다. 자신의 팀이 새로 개발할 시스템이 데이터 저장과 기록, 즉 쓰기에 최적화된 시스템이었기에 작가의 이름을 사용하는 것이 좋겠다고 생각했다. 



링크드인이 카프카를 개발하기까지 어떤 시스템 개선 요구사항이 있었는지 알아보기 위해, 카프카가 개발되기 전인 링크드인의 시스템 구성도를 먼저 보자.



![img](./images/data-flow-ugly-1-1024x570.png)

*카프카가 개발되기 전 링크드인의 데이터 처리 시스템 (출처: https://www.confluent.io/blog/event-streaming-platform-1/)*



시스템이 커지면 링크드인 시스템 구성도와 같이 다음의 여러 데이터 스토어로 분화가 된다.

- 메트릭 모니터링

- 로그 모니터링

- 관계형 DB

  - 앱에서 보낸 OLTP 를 실행

  > ❓ OLTP(On-Line Transaction Processing)
  >
  > 네트워크 상의 여러 이용자가 실시간으로 데이터베이스의 데이터를 갱신하거나 조회하는 등의 단위작업을 처리하는 방식을 말한다.

- 키-벨류 저장소 

  - 추천이나 장바구니 같이 트랜잭션 처리까진 필요 없지만 실시간 처리가 필요

- 데이터 웨어하우스

  - 서비스와 제품군 전체에서 발생하는 데이터를 모아서 일간/주간/월간/연간 데이터를 제공하는 데이터 마켓이나 이것을 활용해 배치 분석

- 하둡 시스템

  - 빅데이터를 저장/처리하기 위함. 빅데이터를 처리해 데이터 웨어하우스에 보내는 ETL 처리 담당

  > ❓ ETL(Extract, Transform, Load)
  >
  > ETL은 추출(Extract), 변환(Transform), 로드(Load)를 나타내며 조직에서 여러 시스템의 데이터를 단일 데이터베이스, 데이터 저장소, 데이터 웨어하우스 또는 데이터 레이크에 결합하기 위해 일반적으로 허용되는 방법. ETL을 사용하여 기존 데이터를 저장하거나 집계(현재 더 일반적인 방식)하여 분석하고 이를 비즈니스 결정에 활용하게 된다. 





#### 기존 시스템 구성의 문제점

1. 실시간 트랜잭션(OLTP) 처리와 비동기 처리가 동시에 이뤄지지만 통합된 전송 영역이 없으니 복잡도가 증가한다. 문제를 발견하고 조치하려면 여러 데이터 시스템을 확인해야한다. 장애나 운영체제 업그레이드, 스케일 아웃 등과 같은 작업을 위해서도 아주 많은 준비 시간이 필요하다.
2. 데이터 파이프라인 관리가 어렵다. 실시간 트랜잭션 데이터베이스, 아파치 하둡, 모니터링 시스템, 키-벨류 저장소 등 많은 데이터 시스템이 있는데, 동일한 데이터를 각 서비스 개발 부서들이 다른 방법으로 파이프라인을 만들고 유지하게 되었다. 처음엔 간편했지만, 데이터 파이프라인들의 통합 데이터 분석을 위해 서로 연결해야되는 일들이 발생했다. 각 파이프라인별 데이터 포맷 및 처리 방법이 달라서 파이프라인 확장이 어려워졌다.
   - 예를 들어 앱의 데이터를 데이터베이스에 특정 스키마로 저장 후 메시지 큐 서비스가 필요하여 추가하려고 한다. 기존 MQ는 성능이 느리고 큰 데이터를 처리할 수 없어 원본 데이터를 줄이고 포맷 변경



#### 카프카의 개발

모든 시스템으로 데이터를 전송할 수 있고, 실시간 처리도 가능하며, 급속도로 성장하는 서비스들을 위해 확장이 용이한 시스템을 만들게 되었다. 

그렇게 만들어진 카프카의 목표는 다음과 같다.

- 프로듀서와 컨슈머의 분리 (의존성 제거)
- 메시징 시스템과 같이 영구 메시지 데이터를 여러 컨슈머에게 허용
- 높은 처리량을 위한 메시지 최적화
- 데이터가 증가함에 따라 스케일 아웃이 가능한 시스템 (리플리케이션, consumer_offset)



링크드인에서는 위의 목표로 카프카를 개발하고 다음과 같은 시스템 구성도로 변경되었다.

![img](./images/data-flow-1024x778.png)

*카프카를 이용한 링크드인의 데이터 처리 시스템 (출처: https://www.confluent.io/blog/event-streaming-platform-1/)*

카프카가 전사 데이터 파이프라인으로 동작함으로서 모든 데이터 스토어와 여기서 발생하는 데이터/이벤트가 카프카 중심으로 연결되었다. 새로운 데이터 스토어가 들어와도 서로 카프카가 제공하는 표준 포맷으로 연결되어있어 데이터를 주고 받는데 부담이 없다.



![커밋 로그](./images/commit_log-copy.png)

*이벤트 레코드 스트림으로서의 카프카 (출처: https://www.confluent.io/blog/event-streaming-platform-1/)*

이젠 카프카에 데이터를 전달만하면 나머지는 필요한 곳 또는 다른 서비스들이 각자 가져갈 수 있어서 자신들 본연의 업무에만 집중할 수 있게 되었다.



## 특징

### 프로듀서와 컨슈머의 분리

링크드인에서는 메트릭 수집 방식을 폴링 방식으로 구현했었다. 메트릭 수집이 늦어지는 경우가 발생하면서 수집이나 처리시간이 늦어지는 문제점이 생겼다. 이를 해결하기 위해 데이터를 보내는 역할과 받는 역할을 분리하게 되었다. 중앙에 메시지 서버를 두고 통신하는 완전한 펍/섭 방식을 채택했다.



![img](./images/img.png)

*펍/섭을 사용하지 않은 경우와 사용한 경우 (출처: https://needjarvis.tistory.com/599)*

만약 **왼쪽 구성**에 서버 1대가 추가된다면, 단순히 서버 한대만 추가하는것이 아니라 연동해야할 시스템이 많아지기 때문에 추가적인 작업도 매우 늘어난다. 일대일 통신하고 있던 모니터링 서버에 문제가 생겨 응답이 느려지면, 연쇄 작용으로 모니터링과 연결된 다른 서비스 서버들에서도 이슈가 발생할 수 있다.

반면에 **오른쪽 펍/섭 구조**는 각자의 역할이 완벽히 분리됨으로서 한쪽 시스템에서 문제가 발생하더라도 연쇄 작용이 발생할 확률이 매우 낮다.



### 멀티 프로듀서, 멀티 컨슈머

![img](./images/img4.png)

*프로듀서와 컨슈머 (출처: https://needjarvis.tistory.com/599?category=925090)*

하나의 프로듀서가 하나의 토픽에만 메시지를 보내는 것이 아니라, 하나 또는 하나 이상의 토픽으로 메시지를 보낼 수 있다. 컨슈머 역시 하나 또는 하나 이상의 토픽으로부터 메시지를 가져올 수 있다. 이러한 멀티 기능은 데이터 분석 및 처리 프로세스에서 하나의 데이터를 다양한 용도로 사용하는 요구가 많아지기 시작했고, 이러한 요구사항을 충족하게 되었다.



### 디스크에 메시지 저장

기존 메시징 시스템과 가장 다른 특징 중 하나가 바로 디스크에 메시지를 저장하고 유지하는 것이다. 메시지 손실 없이 메시지를 가져갈 수 있다. 멀티 컨슈머 또한 카프카에 메시지들이 디스크로 저장되어 있기 때문에 가능하다.

> ❓ 타 메시징 서비스는 디스크에 저장하지 않나? RabbitMQ 



### 확장성

확장에 매우 용이하도록 설계되어 있다. 하나의 카프카 클러스터는 3대의 브로커로 시작해 수십 대의 브로커로 확장 가능하다. 또한 확장 작업은 서비스 중단 없이 온라인 상태에서 작업이 가능하다.



### 높은 성능

내부적으로 분산 처리, 배치 처리 등 다양한 기법을 이용해 고성능을 유지한다는데, 이에 대해서는 추후 문서에서 다루도록 하겠다.



## 확장과 발전

메시징 버스로서의 역할을 넘어 (ESB, 메시지 버스, 이벤트 버스와 같이 기업 내 데이터 흐름을 중앙에서 관리) 카프카 스트림즈, KSQL 를 통해 실시간 분석을 하는 분석 시스템으로서의 역할로도 진화하고 있다. 



![Kafka Inside Keystone Pipeline. the second story in our Keystone… | by  Netflix Technology Blog | Netflix TechBlog](./images/0*jip8Wym1kmlbaNRs..png)

*넷플릭스의 데이터 파이프라인 (출처: https://netflixtechblog.com/kafka-inside-keystone-pipeline-dd5aeabaf6bb?gi=3d117bb17cf7)*



![The ABCs of Kafka in Hyperledger Fabric | by Priyansh Jain | codeburst](./images/0*RvhKCEATyCWmaKrt.png)

*패브릭(Fablic)의 OSN과 카프카 클러스터의 연결 (출처: https://codeburst.io/the-abcs-of-kafka-in-hyperledger-fabric-81e6dc18da56)*

프라이빗 블록체인의 하이퍼렛저의 하위 프로젝트 중 패브릭이라는 프로젝트가 있다. 패브릭은 OSN(Ordering Service Node)라고 불리는 네트워크상의 엔티티가 있는데, OSN의 역할은 모든 트랜잭션을 블록 형태의 메시지로 패키지하고 내부의 카프카 클러스터로 전달한다. 절대로 손실되어서도 안되고, 순서도 보장되어야하는 정보를 카프카를 통해 저장한다.